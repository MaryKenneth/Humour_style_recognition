{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\maryk\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Dataset Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a single Dataset File\n",
    "def read_dataset(file_path):\n",
    "    if file_path.lower().endswith('.csv'):\n",
    "        dataset = pd.read_csv(file_path)\n",
    "    elif file_path.lower().endswith('.xlsx'):\n",
    "        dataset = pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .csv or .xlsx file.\")\n",
    "    \n",
    "    dataset   = np.array(dataset)\n",
    "    data_train, data_test     = train_test_split(dataset, test_size=0.2, random_state=100)\n",
    "\n",
    "    x_train, y_train   = (data_train[:,:-1]).astype(str).tolist(), (data_train[:,-1]).astype(\"int32\").tolist()\n",
    "    x_test, y_test     = (data_test[:,:-1]).astype(str).tolist(), (data_test[:,-1]).astype(\"int32\").tolist()           \n",
    "    #x_train, x_test    = x_train.squeeze(), x_test.squeeze()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# If you have Train and Test Datasets separate\n",
    "def read_train_test_dataset(train_data, test_data):\n",
    "    if train_data.lower().endswith('.csv') and test_data.lower().endswith('.csv'):\n",
    "        train_data = pd.read_csv(train_data)\n",
    "        test_data = pd.read_csv(test_data)\n",
    "    elif train_data.lower().endswith('.xlsx') and test_data.lower().endswith('.xlsx'):\n",
    "        train_data = pd.read_excel(train_data)\n",
    "        test_data = pd.read_excel(test_data)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a .csv or .xlsx file.\")\n",
    "    \n",
    "    train_data, test_data   = np.array(train_data), np.array(test_data)\n",
    "\n",
    "    x_train, y_train   = (train_data[:,:-1]).astype(str).tolist(), (train_data[:,-1]).astype(\"int32\").tolist()\n",
    "    x_test, y_test     = (test_data[:,:-1]).astype(str).tolist(), (test_data[:,-1]).astype(\"int32\").tolist()           \n",
    "    #x_train, x_test    = x_train.squeeze(), x_test.squeeze()\n",
    "\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(train_texts, test_texts):\n",
    "    #Using DistilBert Pre-trained Model\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    train_encodings = tokenizer([text[0] for text in train_texts], truncation=True, padding=True)\n",
    "    test_encodings  = tokenizer([text[0] for text in test_texts], truncation=True, padding=True)\n",
    "\n",
    "    return train_encodings, test_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumourDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.LongTensor([self.labels[idx]])  # Convert to LongTensor\n",
    "\n",
    "        return item  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_parameters(output_dir, logging_dir, num_labels, train_dataset, test_dataset):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,   # output directory\n",
    "        num_train_epochs=5,              \n",
    "        per_device_train_batch_size=8,       # batch size per device during training\n",
    "        per_device_eval_batch_size=64,        # batch size for evaluation\n",
    "        warmup_steps=500,                     # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,                    # strength of weight decay\n",
    "        logging_dir=logging_dir,            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "        eval_dataset=test_dataset            # evaluation dataset\n",
    "    )\n",
    "\n",
    "    return model, trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trained_model, custom_x_test):\n",
    "    trained_model.evaluate()\n",
    "    # Make predictions on the validation dataset\n",
    "    predictions = trained_model.predict(custom_x_test)\n",
    "    predictions = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "    # Extract labels from the validation dataset\n",
    "    labels = custom_x_test.labels\n",
    "    report = classification_report(labels, predictions)\n",
    "\n",
    "    return predictions, report \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model results Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(true_label, predicted):\n",
    "    report_dict = classification_report(true_label,predicted,output_dict=True)\n",
    "\n",
    "    # Save Result Report\n",
    "    save_report = pd.DataFrame(report_dict).transpose()  # Convert the report dictionary to a DataFrame\n",
    "    save_report = save_report.round(3)                   # Round the values to a specific number of decimal places\n",
    "    save_report = save_report.astype({'support': int})   # Convert the 'support' column to integers\n",
    "    save_report.loc['accuracy', ['precision', 'recall', 'support']] = [None, None, None] # Set the accuracy row to None\n",
    "\n",
    "    return save_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8204222b8e4849f686da65fc991c432e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6175, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.31}\n",
      "{'loss': 1.6167, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.62}\n",
      "{'loss': 1.6015, 'learning_rate': 3e-06, 'epoch': 0.94}\n",
      "{'loss': 1.596, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.25}\n",
      "{'loss': 1.5793, 'learning_rate': 5e-06, 'epoch': 1.56}\n",
      "{'loss': 1.5603, 'learning_rate': 6e-06, 'epoch': 1.88}\n",
      "{'loss': 1.532, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.19}\n",
      "{'loss': 1.4593, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.5}\n",
      "{'loss': 1.3572, 'learning_rate': 9e-06, 'epoch': 2.81}\n",
      "{'loss': 1.2215, 'learning_rate': 1e-05, 'epoch': 3.12}\n",
      "{'loss': 1.0806, 'learning_rate': 1.1000000000000001e-05, 'epoch': 3.44}\n",
      "{'loss': 0.9917, 'learning_rate': 1.2e-05, 'epoch': 3.75}\n",
      "{'loss': 0.9669, 'learning_rate': 1.3000000000000001e-05, 'epoch': 4.06}\n",
      "{'loss': 0.7603, 'learning_rate': 1.4000000000000001e-05, 'epoch': 4.38}\n",
      "{'loss': 0.759, 'learning_rate': 1.5e-05, 'epoch': 4.69}\n",
      "{'loss': 0.7044, 'learning_rate': 1.6000000000000003e-05, 'epoch': 5.0}\n",
      "{'loss': 0.6026, 'learning_rate': 1.7000000000000003e-05, 'epoch': 5.31}\n",
      "{'loss': 0.6372, 'learning_rate': 1.8e-05, 'epoch': 5.62}\n",
      "{'loss': 0.5732, 'learning_rate': 1.9e-05, 'epoch': 5.94}\n",
      "{'train_runtime': 7152.6063, 'train_samples_per_second': 0.847, 'train_steps_per_second': 0.027, 'train_loss': 1.1628583334386349, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=192, training_loss=1.1628583334386349, metrics={'train_runtime': 7152.6063, 'train_samples_per_second': 0.847, 'train_steps_per_second': 0.027, 'train_loss': 1.1628583334386349, 'epoch': 6.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "humor_5class_path = \"datasets/Humour_style.xlsx\" \n",
    "x_train_5, x_test_5, y_train_5, y_test_5 = read_dataset(humor_5class_path)\n",
    "train_encodings5, test_encodings5 = tokenizer(x_train_5, x_test_5)\n",
    "train_dataset5 = HumourDataset(train_encodings5, y_train_5)\n",
    "test_dataset5  = HumourDataset(test_encodings5, y_test_5)  \n",
    "output_dir  = 'DistilBERT_Models/distilBERT_5classes'\n",
    "logging_dir = 'DistilBERT_Models/distilBERT_logs_5classes'\n",
    "num_labels= 5\n",
    "model, trainer = train_parameters(output_dir, logging_dir, num_labels, train_dataset5, test_dataset5)\n",
    "trainer.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42161945a1614984b641657a20abf6f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fadb2dd817a47599ef349db35368ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.97      0.83        59\n",
      "           1       0.87      0.71      0.78        48\n",
      "           2       0.67      0.32      0.43        44\n",
      "           3       0.66      0.80      0.73        46\n",
      "           4       0.95      1.00      0.97        56\n",
      "\n",
      "    accuracy                           0.78       253\n",
      "   macro avg       0.78      0.76      0.75       253\n",
      "weighted avg       0.78      0.78      0.76       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save and Evaluate Model \n",
    "model.save_pretrained('DistilBERT_Models/SavedModel_5classes/')\n",
    "predictions5, result5 = evaluate_model(trainer, test_dataset5) \n",
    "print(result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Report\n",
    "distilBERT_result_5= save_results(y_test_5,predictions5)\n",
    "distilBERT_result_5.to_csv('models_results/distilBERT_5classes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e80b650b2747a2ac318e6a5cd4c574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4054, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.31}\n",
      "{'loss': 1.3979, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.62}\n",
      "{'loss': 1.3829, 'learning_rate': 3e-06, 'epoch': 0.94}\n",
      "{'loss': 1.3568, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.25}\n",
      "{'loss': 1.3377, 'learning_rate': 5e-06, 'epoch': 1.56}\n",
      "{'loss': 1.2886, 'learning_rate': 6e-06, 'epoch': 1.88}\n",
      "{'loss': 1.2337, 'learning_rate': 7.000000000000001e-06, 'epoch': 2.19}\n",
      "{'loss': 1.1299, 'learning_rate': 8.000000000000001e-06, 'epoch': 2.5}\n",
      "{'loss': 0.965, 'learning_rate': 9e-06, 'epoch': 2.81}\n",
      "{'loss': 0.8672, 'learning_rate': 1e-05, 'epoch': 3.12}\n",
      "{'loss': 0.7191, 'learning_rate': 1.1000000000000001e-05, 'epoch': 3.44}\n",
      "{'loss': 0.6951, 'learning_rate': 1.2e-05, 'epoch': 3.75}\n",
      "{'loss': 0.6336, 'learning_rate': 1.3000000000000001e-05, 'epoch': 4.06}\n",
      "{'loss': 0.4676, 'learning_rate': 1.4000000000000001e-05, 'epoch': 4.38}\n",
      "{'loss': 0.5455, 'learning_rate': 1.5e-05, 'epoch': 4.69}\n",
      "{'loss': 0.4331, 'learning_rate': 1.6000000000000003e-05, 'epoch': 5.0}\n",
      "{'loss': 0.3986, 'learning_rate': 1.7000000000000003e-05, 'epoch': 5.31}\n",
      "{'loss': 0.3539, 'learning_rate': 1.8e-05, 'epoch': 5.62}\n",
      "{'loss': 0.3324, 'learning_rate': 1.9e-05, 'epoch': 5.94}\n",
      "{'train_runtime': 5829.8027, 'train_samples_per_second': 1.039, 'train_steps_per_second': 0.033, 'train_loss': 0.8859955507020155, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=192, training_loss=0.8859955507020155, metrics={'train_runtime': 5829.8027, 'train_samples_per_second': 1.039, 'train_steps_per_second': 0.033, 'train_loss': 0.8859955507020155, 'epoch': 6.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "humor_4class_path = \"datasets/Humour_style_4classes.xlsx\" \n",
    "x_train_4, x_test_4, y_train_4, y_test_4 = read_dataset(humor_4class_path)\n",
    "train_encodings4, test_encodings4 = tokenizer(x_train_4, x_test_4)\n",
    "train_dataset4 = HumourDataset(train_encodings4, y_train_4)\n",
    "test_dataset4  = HumourDataset(test_encodings4, y_test_4)  \n",
    "output_dir4  = 'DistilBERT_Models/distilBERT_4classes'\n",
    "logging_dir4 = 'DistilBERT_Models/distilBERT_logs_4classes'\n",
    "num_labels4= 4\n",
    "model4, trainer4 = train_parameters(output_dir4, logging_dir4, num_labels4, train_dataset4, test_dataset4)\n",
    "trainer4.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "952347fddfaa4caea33444535ff4f138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5291a0fb3cde47cf80cae3848c4f30b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85        59\n",
      "           1       0.86      0.67      0.75        48\n",
      "           2       0.82      0.90      0.86        90\n",
      "           3       0.97      1.00      0.98        56\n",
      "\n",
      "    accuracy                           0.87       253\n",
      "   macro avg       0.87      0.85      0.86       253\n",
      "weighted avg       0.87      0.87      0.86       253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save and Evaluate Model \n",
    "model4.save_pretrained('DistilBERT_Models/SavedModel_4classes/')\n",
    "predictions4, result4 = evaluate_model(trainer4, test_dataset4) \n",
    "print(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Report\n",
    "distilBERT_result_4= save_results(y_test_4,predictions4)\n",
    "distilBERT_result_4.to_csv('models_results/distilBERT_4classes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fba71a6d9445429307edf743b68342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.719, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n",
      "{'loss': 0.7139, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.41}\n",
      "{'loss': 0.7177, 'learning_rate': 3e-06, 'epoch': 0.61}\n",
      "{'loss': 0.7009, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.82}\n",
      "{'loss': 0.6845, 'learning_rate': 5e-06, 'epoch': 1.02}\n",
      "{'loss': 0.6695, 'learning_rate': 6e-06, 'epoch': 1.22}\n",
      "{'loss': 0.6688, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.43}\n",
      "{'loss': 0.6387, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.63}\n",
      "{'loss': 0.6104, 'learning_rate': 9e-06, 'epoch': 1.84}\n",
      "{'loss': 0.5901, 'learning_rate': 1e-05, 'epoch': 2.04}\n",
      "{'loss': 0.5448, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.24}\n",
      "{'loss': 0.5418, 'learning_rate': 1.2e-05, 'epoch': 2.45}\n",
      "{'loss': 0.5187, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.65}\n",
      "{'loss': 0.4754, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.86}\n",
      "{'loss': 0.4006, 'learning_rate': 1.5e-05, 'epoch': 3.06}\n",
      "{'loss': 0.3862, 'learning_rate': 1.6000000000000003e-05, 'epoch': 3.27}\n",
      "{'loss': 0.3722, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.47}\n",
      "{'loss': 0.2912, 'learning_rate': 1.8e-05, 'epoch': 3.67}\n",
      "{'loss': 0.1809, 'learning_rate': 1.9e-05, 'epoch': 3.88}\n",
      "{'loss': 0.2641, 'learning_rate': 2e-05, 'epoch': 4.08}\n",
      "{'loss': 0.2146, 'learning_rate': 2.1e-05, 'epoch': 4.29}\n",
      "{'loss': 0.1725, 'learning_rate': 2.2000000000000003e-05, 'epoch': 4.49}\n",
      "{'loss': 0.139, 'learning_rate': 2.3000000000000003e-05, 'epoch': 4.69}\n",
      "{'loss': 0.0589, 'learning_rate': 2.4e-05, 'epoch': 4.9}\n",
      "{'train_runtime': 6441.1833, 'train_samples_per_second': 0.299, 'train_steps_per_second': 0.038, 'train_loss': 0.4612898773076583, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=245, training_loss=0.4612898773076583, metrics={'train_runtime': 6441.1833, 'train_samples_per_second': 0.299, 'train_steps_per_second': 0.038, 'train_loss': 0.4612898773076583, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "# Read dataset \n",
    "train_2class_path = \"datasets/af_ag_train.xlsx\" \n",
    "test_2class_path  = \"datasets/af_ag_test.xlsx\" \n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = read_train_test_dataset(train_2class_path, test_2class_path)\n",
    "\n",
    "train_encodings2, test_encodings2 = tokenizer(x_train_2, x_test_2)\n",
    "train_dataset2 = HumourDataset(train_encodings2, y_train_2)\n",
    "test_dataset2  = HumourDataset(test_encodings2, y_test_2)  \n",
    "output_dir2  = 'DistilBERT_Models/distilBERT_2classes'\n",
    "logging_dir2 = 'DistilBERT_Models/distilBERT_logs_2classes'\n",
    "num_labels2= 2\n",
    "model2, trainer2 = train_parameters(output_dir2, logging_dir2, num_labels2, train_dataset2, test_dataset2)\n",
    "trainer2.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5149eabddf0d4e1b83454e5e1b9a5aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4823cf5492e340f2b3d1aca05421fef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.64      0.76        44\n",
      "           1       0.73      0.96      0.83        46\n",
      "\n",
      "    accuracy                           0.80        90\n",
      "   macro avg       0.83      0.80      0.79        90\n",
      "weighted avg       0.83      0.80      0.79        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save and Evaluate Model \n",
    "model2.save_pretrained('DistilBERT_Models/SavedModel_2classes/')\n",
    "predictions2, result2 = evaluate_model(trainer2, test_dataset2) \n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Report\n",
    "distilBERT_result_2= save_results(y_test_2,predictions2)\n",
    "distilBERT_result_2.to_csv('models_results/distilBERT_2classes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
